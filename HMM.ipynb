{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from datetime import datetime, time, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "import mplfinance as mpf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TickerData:\n",
    "    def __init__(self, file_path, start_time=time(8, 0, 0), end_time=time(15, 59, 0)):\n",
    "        self.file_path = file_path\n",
    "        self.start_time = start_time\n",
    "        self.end_time = end_time\n",
    "        self.dataframes = self._load_data()\n",
    "        self.blocks_dataframes = {}\n",
    "\n",
    "    def _load_data(self):\n",
    "        \"\"\"\n",
    "        Loads data from all CSV or TXT files in the specified directory.\n",
    "        Each file's data is stored in a separate DataFrame.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            if not os.path.exists(self.file_path):\n",
    "                print(f\"Error: Path does not exist: {self.file_path}\")\n",
    "                return None\n",
    "\n",
    "            if not os.path.isdir(self.file_path):\n",
    "                print(f\"Error: The path is not a directory: {self.file_path}\")\n",
    "                return None\n",
    "\n",
    "            dataframes = {}\n",
    "            column_names = ['timestamp', 'open', 'high', 'low', 'close', 'volume']\n",
    "\n",
    "            for filename in os.listdir(self.file_path):\n",
    "                file_full_path = os.path.join(self.file_path, filename)\n",
    "\n",
    "                if not os.path.isfile(file_full_path):\n",
    "                    continue\n",
    "\n",
    "                file_ext = os.path.splitext(filename)[1].lower()\n",
    "\n",
    "                if file_ext == '.csv':\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_full_path, delimiter=',')\n",
    "                        df.columns = df.columns.str.strip()\n",
    "                        dataframes[filename] = df\n",
    "                        print(f\"Loaded CSV file: {filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading CSV file {filename}: {e}\")\n",
    "\n",
    "                elif file_ext == '.txt':\n",
    "                    try:\n",
    "                        df = pd.read_csv(file_full_path, delimiter=',', names=column_names, header=None)\n",
    "                        df['volume'] = np.nan\n",
    "                        dataframes[filename] = df\n",
    "                        print(f\"Loaded TXT file: {filename}\")\n",
    "                    except Exception as e:\n",
    "                        print(f\"Error loading TXT file {filename}: {e}\")\n",
    "\n",
    "            if not dataframes:\n",
    "                print(f\"No CSV or TXT files found in directory: {self.file_path}\")\n",
    "                return None\n",
    "\n",
    "            print(f\"Successfully loaded data from {len(dataframes)} files.\")\n",
    "            return dataframes\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading files from directory {self.file_path}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def process_all_data(self):\n",
    "        if not self.dataframes:\n",
    "            print(\"No data to process.\")\n",
    "            return\n",
    "\n",
    "        for filename, df in self.dataframes.items():\n",
    "            print(f\"\\nProcessing data for {filename}\")\n",
    "            self.dataframes[filename] = self.process_data(df)\n",
    "\n",
    "    def process_data(self, df):\n",
    "        # Create an explicit copy to avoid chain indexing warnings\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Convert timestamp to datetime\n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "        # Filter by date range if needed\n",
    "        start_date = pd.to_datetime('2023-09-01')\n",
    "        end_date = pd.to_datetime('2023-09-05')\n",
    "        df = df[(df['timestamp'] >= start_date) & (df['timestamp'] <= end_date)].copy()\n",
    "\n",
    "        # Convert columns to appropriate types\n",
    "        for col in ['open', 'high', 'low', 'close']:\n",
    "            if col in df.columns:\n",
    "                df.loc[:, col] = pd.to_numeric(df[col], errors='coerce')\n",
    "\n",
    "        # Filter by trading hours\n",
    "        df = self.filter_trading_hours(df)\n",
    "\n",
    "        # Add the one-minute range and classify observations\n",
    "        df = self.add_range_and_classify(df)\n",
    "\n",
    "        # Create 5-minute volume blocks and classify them\n",
    "        df = self.classify_volume_blocks(df)\n",
    "\n",
    "        print(f\"Processed data shape: {df.shape}\")\n",
    "        return df\n",
    "\n",
    "\n",
    "    def add_range_and_classify(self, df, threshold=0.1):\n",
    "        \"\"\"\n",
    "        Adds a column for the one-minute range and classifies observations\n",
    "        as Expansion (E), Contraction (C), or Steady (S).\n",
    "        \"\"\"\n",
    "        # Calculate the one-minute range\n",
    "        df['range'] = df['high'] - df['low']\n",
    "\n",
    "        # Initialize the observation column\n",
    "        df['range_obs'] = 'S'  # Default to Steady\n",
    "\n",
    "        # Classify observations based on range changes\n",
    "        for i in range(1, len(df)):\n",
    "            if pd.notna(df.loc[i, 'range']) and pd.notna(df.loc[i - 1, 'range']):\n",
    "                change = df.loc[i, 'range'] - df.loc[i - 1, 'range']\n",
    "                if change > threshold:\n",
    "                    df.loc[i, 'range_obs'] = 'E'  # Expansion\n",
    "                elif change < -threshold:\n",
    "                    df.loc[i, 'range_obs'] = 'C'  # Contraction\n",
    "\n",
    "        return df\n",
    "\n",
    "    def classify_volume_blocks(self, df):\n",
    "        \"\"\"\n",
    "        Classifies each 5-minute block based on volume changes.\n",
    "        \"\"\"\n",
    "        # Create 5-minute blocks\n",
    "        df['block'] = df['timestamp'].dt.floor('5min')\n",
    "        volume_obs = []\n",
    "\n",
    "        for block_time, group in df.groupby('block'):\n",
    "            if len(group) < 5:\n",
    "                continue\n",
    "\n",
    "            volumes = group['volume'].values\n",
    "            min_volume = volumes.min()\n",
    "            max_volume = volumes.max()\n",
    "            avg_volume = volumes.mean()\n",
    "\n",
    "            # Calculate squared distances for each scenario\n",
    "            expanding_volumes = np.linspace(min_volume, max_volume, 5)\n",
    "            contracting_volumes = np.linspace(max_volume, min_volume, 5)\n",
    "            steady_volumes = np.full(5, avg_volume)\n",
    "\n",
    "            expanding_distance = np.sum((volumes - expanding_volumes) ** 2)\n",
    "            contracting_distance = np.sum((volumes - contracting_volumes) ** 2)\n",
    "            steady_distance = np.sum((volumes - steady_volumes) ** 2)\n",
    "\n",
    "            # Determine the classification\n",
    "            if expanding_distance <= contracting_distance and expanding_distance <= steady_distance:\n",
    "                classification = 'E' # 'Expanding'\n",
    "            elif contracting_distance <= expanding_distance and contracting_distance <= steady_distance:\n",
    "                classification = 'C' # 'Contracting'\n",
    "            else:\n",
    "                classification = 'S' # 'Steady'\n",
    "\n",
    "            volume_obs.append((block_time, classification))\n",
    "\n",
    "        # Add classification to DataFrame\n",
    "        classifications_df = pd.DataFrame(volume_obs, columns=['block', 'volume_obs'])\n",
    "        df = df.merge(classifications_df, on='block', how='left')\n",
    "\n",
    "        # Drop the 'block' column\n",
    "        df = df.drop(columns=['block'])\n",
    "\n",
    "        return df\n",
    "\n",
    "    def filter_trading_hours(self, df):\n",
    "        df = df.copy()\n",
    "        df.loc[:, 'time_only'] = df['timestamp'].dt.time\n",
    "        df = df[(df['time_only'] >= self.start_time) & (df['time_only'] <= self.end_time)].copy()\n",
    "        df.drop('time_only', axis=1, inplace=True)\n",
    "        df.reset_index(drop=True, inplace=True)\n",
    "        return df\n",
    "\n",
    "    # def create_10min_blocks_all(self):\n",
    "    #     for filename, df in self.dataframes.items():\n",
    "    #         print(f\"\\nCreating 10-minute blocks for {filename}\")\n",
    "    #         self.blocks_dataframes[filename] = self.create_10min_blocks(df)\n",
    "\n",
    "    def create_10min_blocks_ndx(self):\n",
    "        \"\"\"\n",
    "        Creates 10-minute blocks only for NDX data files.\n",
    "        \"\"\"\n",
    "        for filename, df in self.dataframes.items():\n",
    "            if \"NDX\" in filename:\n",
    "                print(f\"\\nCreating 10-minute blocks for {filename}\")\n",
    "                self.blocks_dataframes[filename] = self.create_10min_blocks(df)\n",
    "\n",
    "\n",
    "    def create_10min_blocks(self, df, window_size=10, step_size=1):\n",
    "        \"\"\"\n",
    "        Creates the rolling 10-minute blocks.\n",
    "        \"\"\"\n",
    "        df_copy = df.copy().sort_values('timestamp')\n",
    "        windows_data = []\n",
    "        \n",
    "        # Define the frequency string based on step_size\n",
    "        freq = f'{step_size}min'\n",
    "        \n",
    "        # Generate start times with the specified step size\n",
    "        start_times = pd.date_range(\n",
    "            df_copy['timestamp'].min(),\n",
    "            df_copy['timestamp'].max() - pd.Timedelta(minutes=window_size),\n",
    "            freq=freq\n",
    "        )\n",
    "        \n",
    "        for start_time in start_times:\n",
    "            end_time = start_time + pd.Timedelta(minutes=window_size)\n",
    "            window_data = df_copy[(df_copy['timestamp'] >= start_time) & (df_copy['timestamp'] < end_time)]\n",
    "            \n",
    "            if window_data.empty:\n",
    "                continue\n",
    "                \n",
    "            window_stats = {\n",
    "                'window_start_time': start_time,\n",
    "                'window_end_time': end_time,\n",
    "                'open': window_data.iloc[0]['open'],\n",
    "                'high': window_data['high'].max(),\n",
    "                'low': window_data['low'].min(),\n",
    "                'close': window_data.iloc[-1]['close'],\n",
    "                'volume': window_data['volume'].sum(),\n",
    "                'range': window_data['high'].max() - window_data['low'].min()\n",
    "            }\n",
    "            \n",
    "            windows_data.append(window_stats)\n",
    "        blocks_df = pd.DataFrame(windows_data)\n",
    "        \n",
    "        # Drop these cols that were used to calc 10mins with missing 1mins\n",
    "        blocks_df.drop(columns=['average_open', 'average_close'], inplace=True, errors='ignore')\n",
    "        \n",
    "        return blocks_df\n",
    "\n",
    "    def classify_ndx_volatility(self):\n",
    "        \"\"\"\n",
    "        Classifies each 10-minute period of the NDX index based on range volatility.\n",
    "        \"\"\"\n",
    "        for filename, blocks_df in self.blocks_dataframes.items():\n",
    "            if \"NDX\" not in filename or blocks_df.empty:\n",
    "                continue\n",
    "\n",
    "            print(f\"Classifying volatility for {filename}\")\n",
    "\n",
    "            # Calculate the range for each 10-minute block\n",
    "            ranges = blocks_df['range'].sort_values()\n",
    "\n",
    "            # Determine thresholds for low, normal, and high volatility\n",
    "            low_threshold = ranges.iloc[len(ranges) // 3]\n",
    "            high_threshold = ranges.iloc[2 * len(ranges) // 3]\n",
    "\n",
    "            # Classify each block based on the range\n",
    "            def classify_range(range_value):\n",
    "                if range_value <= low_threshold:\n",
    "                    return 'L'  # Low volatility\n",
    "                elif range_value <= high_threshold:\n",
    "                    return 'N'  # Normal volatility\n",
    "                else:\n",
    "                    return 'H'  # High volatility\n",
    "\n",
    "            blocks_df['hs_vol'] = blocks_df['range'].apply(classify_range)\n",
    "\n",
    "            # Update the DataFrame with the classification\n",
    "            self.blocks_dataframes[filename] = blocks_df\n",
    "\n",
    "    def plot_1min_data(self):\n",
    "        \"\"\"\n",
    "        Plots the 1-minute data for each security (excluding NDX).\n",
    "        \"\"\"\n",
    "        # Filter out NDX files and get only non-empty dataframes\n",
    "        non_ndx_files = [(name, df) for name, df in self.dataframes.items() \n",
    "                        if \"NDX\" not in name and not df.empty]\n",
    "        \n",
    "        if not non_ndx_files:\n",
    "            print(\"No non-NDX files with 1-minute data to plot.\")\n",
    "            return\n",
    "        \n",
    "        num_plots = len(non_ndx_files)\n",
    "        cols = 3\n",
    "        rows = (num_plots + cols - 1) // cols  # Calculate the number of rows needed\n",
    "        fig, axs = plt.subplots(rows, cols, figsize=(18, 6 * rows))\n",
    "        axs = axs.flatten() if num_plots > 1 else [axs]\n",
    "        plot_index = 0\n",
    "        for filename, df in non_ndx_files:\n",
    "            ohlc_data = df[['timestamp', 'open', 'high', 'low', 'close']].copy()\n",
    "            ohlc_data.set_index('timestamp', inplace=True)\n",
    "            \n",
    "            mpf.plot(ohlc_data, type='candle', style='yahoo',\n",
    "                    ylabel='Price ($)',\n",
    "                    ax=axs[plot_index],\n",
    "                    volume=False)\n",
    "\n",
    "            axs[plot_index].set_title(f'{filename} 1-Minute Data')\n",
    "            axs[plot_index].grid(True)\n",
    "            plot_index += 1\n",
    "\n",
    "        for j in range(plot_index, len(axs)):\n",
    "            fig.delaxes(axs[j])\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig('security_1min_plots.png')\n",
    "        plt.show(block=False)\n",
    "        plt.close(fig)\n",
    "        return\n",
    "\n",
    "    def create_observation_table(self, start_time, end_time):\n",
    "        \"\"\"\n",
    "        Creates a table showing range_obs and volume_obs for each security during the specified time period.\n",
    "        Rows are ticker symbols and columns are timestamps.\n",
    "        \"\"\"\n",
    "\n",
    "        all_observations = pd.DataFrame()\n",
    "        \n",
    "        # Generate time range\n",
    "        time_range = pd.date_range(start=start_time, end=end_time, freq='1min')\n",
    "        time_cols = [t.strftime('%H:%M:%S') for t in time_range]\n",
    "        \n",
    "        # Filter out NDX files\n",
    "        non_ndx_files = {name: df for name, df in self.dataframes.items() if \"NDX\" not in name}\n",
    "        \n",
    "        # For each security, extract the observations\n",
    "        rows = []\n",
    "        for security, df in non_ndx_files.items():\n",
    "            # Extract just the ticker symbol from the filename\n",
    "            ticker = security.split('_')[0]\n",
    "            \n",
    "            # Filter the dataframe to the specified time range\n",
    "            filtered_df = df[(df['timestamp'] >= start_time) & (df['timestamp'] <= end_time)].copy()\n",
    "            \n",
    "            if filtered_df.empty:\n",
    "                continue\n",
    "                \n",
    "            # Create a row for range observations\n",
    "            if 'range_obs' in filtered_df.columns:\n",
    "                range_row = {'Observation': f\"{ticker}_range\"}\n",
    "                filtered_df['time_str'] = filtered_df['timestamp'].dt.strftime('%H:%M:%S')\n",
    "                for time_str in time_cols:\n",
    "                    matching_rows = filtered_df[filtered_df['time_str'] == time_str]\n",
    "                    if not matching_rows.empty:\n",
    "                        range_row[time_str] = matching_rows.iloc[0]['range_obs']\n",
    "                    else:\n",
    "                        range_row[time_str] = None\n",
    "                rows.append(range_row)\n",
    "            \n",
    "            # Create a row for volume observations\n",
    "            if 'volume_obs' in filtered_df.columns:\n",
    "                volume_row = {'Observation': f\"{ticker}_volume\"}\n",
    "                for time_str in time_cols:\n",
    "                    matching_rows = filtered_df[filtered_df['time_str'] == time_str]\n",
    "                    if not matching_rows.empty:\n",
    "                        volume_row[time_str] = matching_rows.iloc[0]['volume_obs']\n",
    "                    else:\n",
    "                        volume_row[time_str] = None\n",
    "                rows.append(volume_row)\n",
    "        \n",
    "        result_table = pd.DataFrame(rows)\n",
    "        if not result_table.empty:\n",
    "            result_table.set_index('Observation', inplace=True)\n",
    "        \n",
    "        return result_table\n",
    "\n",
    "    def get_data(self):\n",
    "        return self.dataframes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded CSV file: TSLA_1min_firstratedata.csv\n",
      "Loaded TXT file: NDX_full_1min.txt\n",
      "Loaded CSV file: MSFT_1min_firstratedata.csv\n",
      "Loaded CSV file: AAPL_1min_firstratedata.csv\n",
      "Loaded CSV file: QQQ_1min_firstratedata.csv\n",
      "Loaded CSV file: VXX_1min_firstratedata.csv\n",
      "Loaded CSV file: AMZN_1min_firstratedata.csv\n",
      "Successfully loaded data from 7 files.\n"
     ]
    }
   ],
   "source": [
    "# Where is your data from the directory that this notebook is in?\n",
    "# Instantiates a class object\n",
    "# Loads the csv and txt files from that directory\n",
    "ticker_data = TickerData(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing data for TSLA_1min_firstratedata.csv\n",
      "Processed data shape: (480, 9)\n",
      "\n",
      "Processing data for NDX_full_1min.txt\n",
      "Processed data shape: (390, 9)\n",
      "\n",
      "Processing data for MSFT_1min_firstratedata.csv\n",
      "Processed data shape: (459, 9)\n",
      "\n",
      "Processing data for AAPL_1min_firstratedata.csv\n",
      "Processed data shape: (480, 9)\n",
      "\n",
      "Processing data for QQQ_1min_firstratedata.csv\n",
      "Processed data shape: (480, 9)\n",
      "\n",
      "Processing data for VXX_1min_firstratedata.csv\n",
      "Processed data shape: (444, 9)\n",
      "\n",
      "Processing data for AMZN_1min_firstratedata.csv\n",
      "Processed data shape: (476, 9)\n"
     ]
    }
   ],
   "source": [
    "# Loads data from files into datframes\n",
    "ticker_data.process_all_data()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Creating 10-minute blocks for NDX_full_1min.txt\n",
      "Classifying volatility for NDX_full_1min.txt\n",
      "\n",
      "10-minute blocks for NDX_full_1min.txt with volatility classification:\n",
      "    window_start_time     window_end_time      open      high       low  \\\n",
      "0 2023-09-01 09:30:00 2023-09-01 09:40:00  15600.95  15618.85  15527.47   \n",
      "1 2023-09-01 09:31:00 2023-09-01 09:41:00  15604.75  15618.85  15514.90   \n",
      "2 2023-09-01 09:32:00 2023-09-01 09:42:00  15596.21  15612.82  15509.57   \n",
      "3 2023-09-01 09:33:00 2023-09-01 09:43:00  15590.66  15612.82  15509.57   \n",
      "4 2023-09-01 09:34:00 2023-09-01 09:44:00  15601.78  15612.82  15509.57   \n",
      "\n",
      "      close  volume   range hs_vol  \n",
      "0  15530.40     0.0   91.38      H  \n",
      "1  15517.11     0.0  103.95      H  \n",
      "2  15510.85     0.0  103.25      H  \n",
      "3  15520.01     0.0  103.25      H  \n",
      "4  15528.97     0.0  103.25      H  \n"
     ]
    }
   ],
   "source": [
    "# Creates rolling 10min block for the NDX\n",
    "ticker_data.create_10min_blocks_ndx()\n",
    "\n",
    "# Calculates the hidden states (target) from the 10min block range size\n",
    "ticker_data.classify_ndx_volatility()\n",
    "\n",
    "# Display the 10-minute blocks for NDX with hidden state volatility classification\n",
    "for filename in ticker_data.blocks_dataframes:\n",
    "    if \"NDX\" in filename:\n",
    "        blocks_df = ticker_data.blocks_dataframes[filename]  # Get fresh reference\n",
    "        print(f\"\\n10-minute blocks for {filename} with volatility classification:\")\n",
    "        try:\n",
    "            print(blocks_df[['window_start_time', 'window_end_time', 'open', 'high', 'low', 'close', 'volume', 'range', 'hs_vol']].head())\n",
    "        except KeyError as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            print(\"Available columns:\", blocks_df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            09:30:00 09:31:00 09:32:00 09:33:00 09:34:00 09:35:00 09:36:00  \\\n",
      "Observation                                                                  \n",
      "TSLA_range         E        C        E        C        E        S        E   \n",
      "TSLA_volume        C        C        C        C        C        S        S   \n",
      "MSFT_range         E        C        C        C        E        C        E   \n",
      "MSFT_volume        C        C        C        C        C        C        C   \n",
      "AAPL_range         E        S        S        C        E        C        S   \n",
      "AAPL_volume        C        C        C        C        C        C        C   \n",
      "QQQ_range          E        E        C        S        E        C        S   \n",
      "QQQ_volume         C        C        C        C        C        S        S   \n",
      "VXX_range          S        S        S        S        S        S        S   \n",
      "VXX_volume         C        C        C        C        C        S        S   \n",
      "AMZN_range         E        S        E        C        E        S        S   \n",
      "AMZN_volume        C        C        C        C        C        C        C   \n",
      "\n",
      "            09:37:00 09:38:00 09:39:00  ... 09:51:00 09:52:00 09:53:00  \\\n",
      "Observation                             ...                              \n",
      "TSLA_range         C        S        E  ...        E        C        S   \n",
      "TSLA_volume        S        S        S  ...        C        C        C   \n",
      "MSFT_range         C        S        C  ...        C        S        E   \n",
      "MSFT_volume        C        C        C  ...        C        C        C   \n",
      "AAPL_range         S        S        S  ...        S        C        S   \n",
      "AAPL_volume        C        C        C  ...        C        C        C   \n",
      "QQQ_range          C        C        S  ...        S        C        E   \n",
      "QQQ_volume         S        S        S  ...        C        C        C   \n",
      "VXX_range          S        S        S  ...        S        S        S   \n",
      "VXX_volume         S        S        S  ...        C        C        C   \n",
      "AMZN_range         C        E        C  ...        S        C        S   \n",
      "AMZN_volume        C        C        C  ...        C        C        C   \n",
      "\n",
      "            09:54:00 09:55:00 09:56:00 09:57:00 09:58:00 09:59:00 10:00:00  \n",
      "Observation                                                                 \n",
      "TSLA_range         E        C        C        E        E        C        E  \n",
      "TSLA_volume        C        S        S        S        S        S        C  \n",
      "MSFT_range         C        S        C        S        S        S        E  \n",
      "MSFT_volume        C        E        E        E        E        E        C  \n",
      "AAPL_range         S        S        C        S        S        S        E  \n",
      "AAPL_volume        C        S        S        S        S        S        C  \n",
      "QQQ_range          S        S        C        S        S        S        E  \n",
      "QQQ_volume         C        S        S        S        S        S        C  \n",
      "VXX_range          S        S        S        S        S        S        E  \n",
      "VXX_volume         C        S        S        S        S        S        S  \n",
      "AMZN_range         S        S        C        S        S        S        E  \n",
      "AMZN_volume        C        S        S        S        S        S        C  \n",
      "\n",
      "[12 rows x 31 columns]\n"
     ]
    }
   ],
   "source": [
    "# Define the time range we want to display\n",
    "start_time = pd.Timestamp('2023-09-01 09:30:00')\n",
    "end_time = start_time + timedelta(minutes=30)\n",
    "\n",
    "# Create and display the observation table\n",
    "observation_table = ticker_data.create_observation_table(start_time, end_time)\n",
    "print(observation_table)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py313",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
